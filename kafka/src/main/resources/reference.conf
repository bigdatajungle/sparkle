kafka-loader {
  // set to true to automatically the kafka loader at startup
  auto-start = false
  
  kafka-reader {   // these are copied verbatim to the kafka consumer see http://kafka.apache.org/08/configuration.html
    zookeeper.connect = "localhost:2181"    
    zookeeper.session.timeout.ms = 4000
    zookeeper.sync.time.ms = 200
    auto.offset.reset = smallest
    auto.commit.interval.ms = 1000
    auto.commit.enable = false
    consumer.timeout.ms = 10000
  }
  reader {
    consumer-group = "defaultConsumer"
  }
  
  kafka-writer {   // these are copied verbatim to the kafka producer see http://kafka.apache.org/08/configuration.html 
    metadata.broker.list = "localhost:9092"
    request.required.acks = 1
  }
  
  // list of topics to stream load
  topics = []
  
  // fully qualified class name that will identify a KafkaColumnDecoder for each topic kafka topic. 
  // The class must have a zero argument constructor and implement the trait FindDecoder.
  find-decoder = ""
  
  log4j {
    // path and log file name 
    file = "kafka-client.log"    
    
    // whether to start a new log file with each execution 
    append = false    
    
    // log4j log pattern
    pattern = "%d{MM/dd HH:mm:ss.SSS} %-5p %c - %m%n" 
    
    console-levels {
      kafka.producer.BrokerPartitionInfo = ERROR
      kafka.producer.async.DefaultEventHandler = OFF
    }
  }
  
}
