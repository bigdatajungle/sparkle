sparkle-time-server { // TODO rename to sparkle
    
  logging {
    provider = log4j  // Kakfa & zookeeper REQUIRE using log4j
    
    levels {
      org.apache.zookeeper = WARN
      org.I0Itec.zkclient = WARN
      kafka.producer.BrokerPartitionInfo = ERROR
      kafka.producer.async.DefaultEventHandler = OFF
    }

    file {
      path = "/tmp/kafka-loader.log"
    }
  }
  
  measure {
    metrics-gateway {
      enable = false
    }
    
    tsv-gateway {
      enable = false
      file = "/tmp/kafka-loader-measurements.tsv"
    }
  }
  
  spray.can.server {
    idle-timeout    = 180 s
    request-timeout = 120 s  // admin calls take awhile
  }

  kafka-loader {
    // Number of kafka messages to read before committing
    message-commit-limit = 1000
    
    // maximumn number of simultaneous Cassandra writes for each topic loaded
    max-concurrent-writes = 500
    
    kafka-reader {   // these are copied verbatim to the kafka consumer see http://kafka.apache.org/08/configuration.html
      zookeeper.connect = "localhost:2181"
      zookeeper.connection.timeout.ms = 10000
      zookeeper.session.timeout.ms = 30000
      zookeeper.sync.time.ms = 2000
      auto.offset.reset = smallest
      auto.commit.interval.ms = 1000
      auto.commit.enable = false
      consumer.timeout.ms = 500
      fetch.message.max.bytes = 10485760
    }
    
    reader {
      // the kafka reader uses a consumer group per topic to avoid issues restarting the 
      consumer-group-prefix = "defaultConsumer"
    }
    
    kafka-writer {
      producer { // these are copied verbatim to the kafka producer see http://kafka.apache.org/08/configuration.html 
        metadata.broker.list = "localhost:9092"
        request.required.acks = 1
        message.send.max.retries = 20
        retry.backoff.ms = 200
        // TODO try async later for faster batched sends.. 
        //      producer.type = async  
        //      queue.buffering.max.ms = 500
      }
      
      send-retry-wait = 10 ms
      send-max-retries = 20
    }
    
    // list of topics to stream load
    topics = []
    
    // list of transformers to apply as data is stream-loaded
    // each transformer should be an object of this form:
    // {  
    //    match = "^MyTopicName.*",                 // regex to match the topic name 
    //    transformer = "mine.domain.MyFilterClass" // transformer class to instantiate
    // }
    // 
    // The provided transformer class should extend LoadingTransformer and should have a single argument
    // constructor that takes a Config parameter
    //
    transformers = []
    
    // fully qualified class name that will identify a KafkaColumnDecoder for each topic kafka topic. 
    // The implementing subclass must implement the trait FindDecoder and it must have a single argument 
    // constructor that takes a Config parameter
    find-decoder = ""
  }
}
